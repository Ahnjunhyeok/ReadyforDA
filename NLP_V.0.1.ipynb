{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 자연어 처리 및 분석에 대한 이해\n",
    "    \n",
    "    - 자연어 처리에 대한 기본적인 내용\n",
    "    - 자연어 처리 연산\n",
    "    - 형태소 분석과 개체명 인식\n",
    "    - 불용어 처리 / 품사 추출\n",
    "    - 빈도수 계산을 통한 워드 클라우드"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 자연어 처리 연산\n",
    "    - 1. Edit Distance : 2개의 단어가 얼마나 다른지 숫자로 나타낸 척도\n",
    "        - Cat / Hat 의 Edit Distance : 1\n",
    "        - 단어의 유사성을 파악할 때 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Natural Language Tool-kit : 영어 및 통계기호의 자연어 처리 라이브러리\n",
    "from nltk.metrics import edit_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance(\"Cat\", \"Hat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance(\"Console\", \"Counsel\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2. Stemming : 어간 추출, 단어의 본래 형태로 추출\n",
    "    - going -> go\n",
    "    - Computer -> compute\n",
    "    - 문장의 의도 파악을 위해 처리하는 작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'comput'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst = PorterStemmer()\n",
    "pst.stem(\"Computer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pst.stem(\"Going\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 3. Lemmatization : 문장 속에서 다양한 형태로 활용된 단어의 표제어를 찾는 작업\n",
    "    - 표제어란? 사전에 등록되어 있는 단어의 기본형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ahnjh\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"wordnet\") # 단어의 원형이 들어있는 사전 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eat'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlem = WordNetLemmatizer()\n",
    "wlem.lemmatize(\"ate\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlem.lemmatize(\"is\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'be'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wlem.lemmatize(\"are\", pos=\"v\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\ahnjh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Show', 'me', 'the', 'money']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 어절 분리(Word Seperation)\n",
    "text1 = \"Show me the money\"\n",
    "\n",
    "nltk.word_tokenize(text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Ahn Jun Hyeok.',\n",
       " \"What's your name?\",\n",
       " 'How are you?',\n",
       " 'Fine, Thank you.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 문장 분리(Sentence Seperation)\n",
    "text2 = \"My name is Ahn Jun Hyeok. What's your name? How are you? Fine, Thank you.\"\n",
    "\n",
    "nltk.sent_tokenize(text2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'letter',\n",
       " ',',\n",
       " 'which',\n",
       " 'orginated',\n",
       " 'in',\n",
       " 'England',\n",
       " ',',\n",
       " 'rotates',\n",
       " 'once',\n",
       " 'a',\n",
       " 'year',\n",
       " 'to',\n",
       " 'give',\n",
       " 'good',\n",
       " 'luck',\n",
       " 'to',\n",
       " 'the',\n",
       " 'recipient',\n",
       " ',',\n",
       " 'and',\n",
       " 'now',\n",
       " 'transfered',\n",
       " 'to',\n",
       " 'you',\n",
       " ',',\n",
       " 'must',\n",
       " 'leave',\n",
       " 'you',\n",
       " 'within',\n",
       " '4',\n",
       " 'days',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text3 = \"This letter, which orginated in England, rotates once a year to give good luck to the recipient, and now transfered to you, must leave you within 4 days.\"\n",
    "tokens = nltk.word_tokenize(text3)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\ahnjh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tagging\n",
    "nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('letter', 'NN'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('orginated', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('England', 'NNP'),\n",
       " (',', ','),\n",
       " ('rotates', 'VBZ'),\n",
       " ('once', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('year', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('give', 'VB'),\n",
       " ('good', 'JJ'),\n",
       " ('luck', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('recipient', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('transfered', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('you', 'PRP'),\n",
       " (',', ','),\n",
       " ('must', 'MD'),\n",
       " ('leave', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('within', 'IN'),\n",
       " ('4', 'CD'),\n",
       " ('days', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\ahnjh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\ahnjh\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 개체명 인식\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 'DT'),\n",
       " ('letter', 'NN'),\n",
       " (',', ','),\n",
       " ('which', 'WDT'),\n",
       " ('orginated', 'VBD'),\n",
       " ('in', 'IN'),\n",
       " ('England', 'NNP'),\n",
       " (',', ','),\n",
       " ('rotates', 'VBZ'),\n",
       " ('once', 'RB'),\n",
       " ('a', 'DT'),\n",
       " ('year', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('give', 'VB'),\n",
       " ('good', 'JJ'),\n",
       " ('luck', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('the', 'DT'),\n",
       " ('recipient', 'NN'),\n",
       " (',', ','),\n",
       " ('and', 'CC'),\n",
       " ('now', 'RB'),\n",
       " ('transfered', 'VBN'),\n",
       " ('to', 'TO'),\n",
       " ('you', 'PRP'),\n",
       " (',', ','),\n",
       " ('must', 'MD'),\n",
       " ('leave', 'VB'),\n",
       " ('you', 'PRP'),\n",
       " ('within', 'IN'),\n",
       " ('4', 'CD'),\n",
       " ('days', 'NNS'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged1 = nltk.pos_tag(tokens)\n",
    "tagged1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  This/DT\n",
      "  letter/NN\n",
      "  ,/,\n",
      "  which/WDT\n",
      "  orginated/VBD\n",
      "  in/IN\n",
      "  (GPE England/NNP)\n",
      "  ,/,\n",
      "  rotates/VBZ\n",
      "  once/RB\n",
      "  a/DT\n",
      "  year/NN\n",
      "  to/TO\n",
      "  give/VB\n",
      "  good/JJ\n",
      "  luck/NN\n",
      "  to/TO\n",
      "  the/DT\n",
      "  recipient/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  now/RB\n",
      "  transfered/VBN\n",
      "  to/TO\n",
      "  you/PRP\n",
      "  ,/,\n",
      "  must/MD\n",
      "  leave/VB\n",
      "  you/PRP\n",
      "  within/IN\n",
      "  4/CD\n",
      "  days/NNS\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "enitities = nltk.chunk.ne_chunk(tagged1)\n",
    "print(enitities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text5 = \"Mark is studying at Stanford University in California\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON Mark/NNP)\n",
      "  is/VBZ\n",
      "  studying/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION Stanford/NNP University/NNP)\n",
      "  in/IN\n",
      "  (GPE California/NNP))\n"
     ]
    }
   ],
   "source": [
    "token1 = nltk.word_tokenize(text5)\n",
    "tagged2 = nltk.pos_tag(token1)\n",
    "enitities2 = nltk.chunk.ne_chunk(tagged2)\n",
    "print(enitities2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. 한국어 자연어 처리\n",
    "    - 한국어 형태소 분석기\n",
    "    - 형태소 분석 기반의 워드 클라우드\n",
    "    - 단어 표현\n",
    "    - 문장 표현\n",
    "    - 전통 통계학 텍스트 분류(기계학습을 이용한 텍스트 분류)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. 신경망 알고리즘을 활용한 텍스트 마이닝\n",
    "    - RNN 기반의 분석\n",
    "    - LSTM / GRU Model\n",
    "    - 순환 드롭아웃 / 스태킹 순환 Layer / 양방향 RNN\n",
    "    - 텍스트 유사도 분석, 네트워크 분석"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
